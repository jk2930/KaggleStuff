{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I performed exploratory data analysis of the Covid 19 Case Surveillance Public Use Dataset, and then used the data to create models to predict patient outcomes given the types of data included in the dataset.\n\n## Findings\n\n### Missing Data\nI began by looking at the unique values inputted for each variable.  I found there were very high rates of data entries of \"Missing\" and \"Unknown\".  I knew rows with missing values must be removed for some pieces of analysis, but I found this curious, so I investigated which patients had the most missing values. \n\nI found that less data was missing for older patients compared to younger patients. I suspect the reason for this is related to severity of disease cases, as well as perhaps the uncertainty of young patients having relevant pre-existing medical conditions. \n\nI also found differences in the amount of missing data when looking at patient race/ethnicity.  Here, I supposed the missing data may be related to regional hospital practices, or hospital stress (i.e., being under-staffed). \n\n### Predictive Models\n\nI continued on to create predictive models.  I used the caret package to make training and testing datasets from the patients who did not have missing/unknown entries.  I compared three different models in predicting patient hospitalization.  The models were base R binomial GLM, caret Bayes GLM, and caret rpart decision tree.  I compared these models by their accuracy, their ROC curves and AUCs, and their run-times.  While both GLM models had similar ROC curves and accuracies, the base R binomial GLM is significantly faster.  Therefore, I continued with this modeling technique to predict patient death.  \n\nThe hospitalization predictions had a 83% accuracy, and death predictions had ~95% accuracy.  This held fairly constant for the training and testing sets.  I have concluded it is a reasonable model for the data.\n\n## Additional Considerations\n\nUpon learning that a friend of mine had a recent exposure to Covid-19, I decided to run the models on his case.  It seems worthwhile to mention here, as I told my friend, that this data set is limited.  This is only data for patients for who there is data.  There is no data on the large number of asymptomatic and very mild cases among people who never thought to get a Covid-19 test or seek medical attention.  Therefore, the probability of an outcome such as hospitalization or death is very likely lower than that which any of these models can predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"library(tictoc)\ntic()\ndata <- read.csv('../input/covid19-case-surveillance-public-use-dataset/COVID-19_Case_Surveillance_Public_Use_Data.csv')\n#head(data)\ndat <- data[, 4:11]\n#head(dat)\ntoc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename columns for ease\ncolnames(dat) <- c(\"current_status\", \"sex\", \"age\", \"race_ethnicity\", \"hosp\", \"icu\", \"death\", \"medcond\")\n\n# determine unique vals in each column to identify those without information, or of insufficient frequency (e.g., sex == \"Other\")\nfor (col in 1:8){\n    print(colnames(dat)[col])\n    print(as.data.frame(table(dat[,col])))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Dataframe without Missing/Unknown/Low-count values\ndf <- dat %>% filter(sex %in% c(\"Female\", \"Male\"),\n                     age != \"Unknown\",\n                     race_ethnicity != \"Unknown\" & race_ethnicity != \"Missing\",\n                     hosp %in% c(\"Yes\", \"No\"),\n                     icu %in% c(\"Yes\", \"No\"),\n                     death %in% c(\"Yes\", \"No\"),\n                     medcond %in% c(\"Yes\", \"No\"))\nfor (col in 1:8){\n    print(colnames(df)[col])\n    print(as.data.frame(table(df[,col])))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndim(df)\n#dim(dat)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This filtration reduced the dataframe from 8e6 to 4e5 rows.  The remaineder is still sufficient to perform analysis.\n\nHowever, this is a dramatic reduction in data.  Which demographic groups have lost the most entries?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# investigate age bias\nAfiltering <- cbind(as.data.frame(table(dat$age)),as.data.frame(table(df$age))[,2] )\ncolnames(Afiltering) <- c(\"age\", \"Freq1\", \"Freq2\" )\nAfiltering$percent_lost <- 100 * (1 - Afiltering[,3] / Afiltering[,2])\nAfiltering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Akeeplose <- cbind(Afiltering$Age, Afiltering[,3], #keep\n                    (Afiltering[,2] - Afiltering[,3]))[-10,] #lost, and removing 'Unknown' row\nAkeeplose\nprop.test(as.matrix(Akeeplose), correct = F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# investigate race_ethnic bias\nREfiltering <- cbind(as.data.frame(table(dat$race_ethnicity)),as.data.frame(table(df$race_ethnicity))[,2] )\ncolnames(REfiltering) <- c(\"race_ethnicity\", \"Freq1\", \"Freq2\" )\nREfiltering$percent_lost <- 100 * (1 - REfiltering[,3] / REfiltering[,2])\nREfiltering\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Afilter<- Afiltering[1:9, c(1,4)]\nREfilter <- REfiltering[c(-5,-8), c(1,4)]\nAfilter\nREfilter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RE_outcomes <- dat %>% group_by(race_ethnicity) %>% summarise(p_hosp = 100*mean(hosp == \"Yes\"),\n                                                              p_icu = 100*mean(icu== \"Yes\"),\n                                                              p_death = 100*mean(death == \"Yes\"))\n\nAge_outcomes <- dat %>% group_by(age) %>% summarise(p_hosp = 100*mean(hosp == \"Yes\"), \n                                                    p_icu = 100*mean(icu== \"Yes\"),\n                                                    p_death = 100*mean(death == \"Yes\"))\nRE_outcomes <- as.data.frame(RE_outcomes)[c(-5,-8,-10),]\nAge_outcomes <- as.data.frame(Age_outcomes)[1:9,]\nRE_outcomes\nAge_outcomes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RE_outcomes <- merge(RE_outcomes, REfilter, by = \"race_ethnicity\")\nAge_outcomes <- merge(Age_outcomes, Afilter, by = \"age\")\nRE_outcomes\nAge_outcomes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot age & outcomes\na <- Age_outcomes\nplot(a$percent_lost, a$p_hosp, type = \"b\", col = \"green\", lwd = 2,\n     xlab = \"Percent with Missing/Unknown Data\", \n     ylab = \"Percent of Total With Outcome\", main = \"Patient Outcomes by Age and Percent Missing Data\", \n    ylim=c(-1, 30))\nlines(a$percent_lost, a$p_icu, type = \"b\", lwd = 2,col = \"red\")\nlines(a$percent_lost, a$p_death, type = \"b\", lwd = 2,col = \"black\")\nlegend(93.6,28, legend=c(\"Hospital\", \"ICU\", \"Death\"),\n       col=c(\"green\", \"red\", \"black\"), lty=1, bty=\"n\", cex = 1.3)\nabline(v=a$percent_lost, col=c(\"grey\"), lty=c(1))\ncoords <- c(5, 16, 12, 20, 7,9, 5, 15, 11)\nfor (i in 1:9){\n    text(a$percent_lost[i], coords[i], toString(a$age[i]), pos = c(4,3,2,4,2,3,3,3,3), srt=90)\n}\n\n# Plot Race/Ethnicity & Outcomes\nr <- RE_outcomes[order(RE_outcomes$percent_lost),]\nplot(r$percent_lost, r$p_hosp, type = \"b\", col = \"green\", lwd = 2,\n     xlab = \"Percent with Missing/Unknown Data\", \n     ylab = \"Percent of Total With Outcome\", \n    ylim=c(-1, 18),\n    main = \"Patient Outcomes by Race/Ethnicity and Percent Missing Data\",)\nlines(r$percent_lost, r$p_icu, type = \"b\", lwd = 2,col = \"red\")\nlines(r$percent_lost, r$p_death, type = \"b\", lwd = 2,col = \"black\")\nlegend(95,19, legend=c(\"Hospital\", \"ICU\", \"Death\"),\n       col=c(\"green\", \"red\", \"black\"), lty=1, bty=\"n\")\nabline(v=r$percent_lost, col=c(\"grey\"), lty=c(1))\n#coords <- c(5, 15, 12, 20, 8,9, 5, 15, 11)\nfor (i in 1:9){\n    text(r$percent_lost[i], r$p_hosp[i], toString(r$race_ethnicity[i]), pos = c(4,4,4,4,1,4,4), srt=90, cex = .7)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Less data was acquired in younger patients.  Perhaps this was due to less information about, say, pre-existing medical conditions, and fewer younger patients required the more advanced care.\n\nWe can see here there is a negative relationship between age groups with missing entries in the data and hospital/ICU/death rates.\n\nHowever, when considering race/ethnic groups, there is not such a relationship.  Some race/ethnic groups had significantly more missing data than others.  Disease outcomes, most notably hospitalization, also varied greatly across race/ethnic groups. \n\nSome patterns of note:\n* People categorized as Asian or Black have higher rates of poor disease outcomes.\n* People categorized as Native Hawaiian/Pacific Islander have lower rates of missing data.  This may be due to lower rates of covid in Hawaii, resulting in a better capacity for hospital staff to record more details of patients.\n* People categorized as American Indian/ Alaskan Native had the highest rate of missing data, at 97%, more than 2% more than the next group (race/ethnicity recorded as Multiple/Other). This may also be connected to regional hospital staff capacity to record detailed data.\n\n"},{"metadata":{},"cell_type":"markdown","source":"The proportion test provides a p-value of 2.2e-16, which suggests that the differences in race/ethnic groups' rates of missing data are not due to chance.  Of course, this result is not surprising, as the identical investigation into differences in data completion for age groups found a differences within 1-2% to also not be due to chance.  However, it is noteworthy that the data discrepancies between age groups are much smaller than those for ethnic groups.  Additionally, the age groups most at risk for disease complications (older decades) had more thorough records, while the opposite is true regarding risk level and race/ethnicity (see models below).\n\nPossible explanations for missing data:\n* communication/language barriers\n* hospital recording methods\n* hospital recording dilligence (keeping in mind how many hospitals have been short of staff, etc.)\n\nIt is unfortunate to have such discrepancies in the data.  We may consider minimizing these.  There is more 'Missing' or 'Unknown' data from the ICU variable and the MedCond variable than any other.  \n\nRegarding the ICU variable, I suspect that different hospitals had different methods for whether or not this data was recorded/included.  Next I will investigate whether the ICU variable provides more information than Hosp or Death.  In other words, does the ICU data have a strong additional influence on predictions?\n\nRegarding Medcond, there is no alternative to this information, but later we can investigate if it is a strong predictor of patient outcomes.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Hosp, ICU, & Death for rows in which there is no missing data.\nHID <- df[,5:7]\n#unique(HID) # check all entries are 'Yes' or 'No'\nHID <- ifelse(HID == 'Yes', 1, 0)\n\ncor(HID[,1], HID[,2])\ncor(HID[,2], HID[,3])\ncor(HID[,1], HID[,3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find a modest correlation between hosp & ICU and between death & ICU.  There is an even lower correlation between hosp and death (which is intuitive).  However, this leaves a moderate predictive power for hosp or death to take the place of the missing ICU entries.\n\nGiven this, I will continue with the analysis only for the non-missing/unknown data.  I will do a similar investigation with the removal of ICU and Medcond as predictors to include a larger amount of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://datascienceplus.com/perform-logistic-regression-in-r/\n\nlibrary(caret)\n#df <- ifelse(df[5:7] == 'Yes', 1, 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# factorize variables to remove \"Missing\" and \"Unknown\" levels\nfor (i in 1:8){\n    df[,i] <- factor(df[,i])\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict hosp\nset.seed(100)\nintrain<-createDataPartition(y=df$hosp,p=0.7,list=FALSE)\ntraining<-df[intrain,]\ntesting<-df[-intrain,]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# caret bayesglm model\n\n# this takes a long time to run\n# age 20 - 69 -higher pval\n# 70s, 80s strongest predictor (by coefficient)\n# all race_eth negative coef besides Native, which is also the group with the \n# least detailed data collection.  Perhaps outcomes are more closely related to hospital conditions \n# compared to race_ethn.  Similarly, being white reduces the most\ntic()\nbayes_hosp_model <- train(hosp ~ age + sex + race_ethnicity + medcond - 1, \n                 method = \"bayesglm\",data=training)\ntoc() # ~200 seconds\n# this does nothing, but why?  preobj <- preProcess(training[,-5],method = c(\"pca\"), thresh = .9)\n#preobj$numComp\n\nsummary(bayes_hosp_model)\n# model accuracy\nconfusionMatrix(predict(bayes_hosp_model, training[,]), reference=training$hosp) # Accuracy 84%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# decision tree \n# note: cannot run random forest on my computer\ntic()\ncaret_rpart_hosp_model <- train(hosp ~ age + sex + race_ethnicity + medcond - 1, \n                 method = \"rpart\",data=training)\ntoc() # ~100 sec\n#summary(caret_rpart_hosp_model)\n\nconfusionMatrix(predict(caret_rpart_hosp_model, training[,]), reference=training$hosp) #also ~84%\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GLM predict hosp\ntic()\nglm_hosp_model <- glm(hosp ~ factor(age) + factor(sex) + factor(race_ethnicity) + medcond - 1, \n                  family=\"binomial\",data=training)\ntoc() # ~2 sec\nsummary(glm_hosp_model)\n\n#test_glm_hosp <- predict(glm_hosp_model, testing[,], type = \"response\")\ntrain_pred <- predict(glm_hosp_model, training[,-5], type = \"response\")\n#summary(train_pred)\n\nbinary_train_pred <- ifelse(train_pred > 0.5,1,0)\n#summary(train_pred)\n\n\nbinary_train_hosp <- ifelse(training$hosp == \"Yes\",1,0)\nlevels(binary_train_hosp) <- c(0,1) \nlevels(binary_train_pred) <- c(0,1) \n\nbinary_train_hosp <- factor(binary_train_hosp)\nbinary_train_pred <- factor(binary_train_pred)\n\nconfusionMatrix(binary_train_pred, reference=binary_train_hosp) #also ~84%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC evaluation\nlibrary(ROCR)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GLM ROC\npr <- prediction(train_pred, binary_train_hosp)\nprf <- performance(pr, measure = \"tpr\", x.measure = \"fpr\")\nplot(prf, main = \"GLM ROC, AUC = .828, Accuracy = 84%\")\n\nauc <- performance(pr, measure = \"auc\")\nauc <- auc@y.values[[1]]\nauc # 0.828","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rpart Decision Tree ROC\n\ndt_pred <- predict(caret_rpart_hosp_model, training[,], type = \"prob\")\n#head(dt_pred)\n\ndt_pr <- prediction(dt_pred[,2],training$hosp)\ndt_prf <- performance(dt_pr, measure = \"tpr\", x.measure = \"fpr\")\nplot(dt_prf, main = \"Decision Tree ROC, AUC = .745, Accuracy = 84%\")\n\ndt_auc <- performance(dt_pr, measure = \"auc\")\ndt_auc <- dt_auc@y.values[[1]]\ndt_auc # 0.745\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# caret Bayes GLM ROC\n\nbayes_pred <- predict(bayes_hosp_model, training[,], type = \"prob\")\n#head(bayes_pred)\n\nbayes_pr <- prediction(bayes_pred[,2],training$hosp)\nbayes_prf <- performance(bayes_pr, measure = \"tpr\", x.measure = \"fpr\")\nplot(bayes_prf, main = \"Bayes GLM ROC, AUC = .828, Accuracy = 84%\")\n\nb_auc <- performance(bayes_pr, measure = \"auc\")\nb_auc <- b_auc@y.values[[1]]\nb_auc # 0.828\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot( bayes_prf, col = \"blue\", lwd = 2, alpha = .8, main = \"ROC Comparisons: All Models Had Accuracy ~84%\")\nplot(prf, add = TRUE, col= \"orange\", lty= \"dashed\", lwd = 2, alpha = .2)\nplot(dt_prf, add = TRUE, col= \"green\", lwd = 2)\nlegend(0.2, 0.4, legend=c(\"Bayes GLM: AUC = .83\", \"GLM: AUC = .83\", \"Dec Tree: AUC = .75\"),\n       col=c(\"blue\", \"orange\", \"green\"), lty=c(1,1,1), lwd = 2, bty = \"n\", cex = 1.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two GLM models perform equally well, and are better than the Decision Tree for predicting a hospital outcome from demographic and medical condition data.  The Bayes GLM model under the caret package had a processing time of approximately 200 seconds.  The other GLM model had a processing time of two seconds, so this is a more efficient way to generate the linear model.\n\nContinuing with this GLM model, we can evaluate how data on medical conditions and ICU stays influences the model in predicting patient death."},{"metadata":{"trusted":true},"cell_type":"code","source":"head(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GLM predict death\n\n# predict with hosp, icu, & medcond\ntic()\nglm_death_model <- glm(death ~ factor(age) + factor(sex) + factor(race_ethnicity) + hosp + icu+ medcond - 1, \n                  family=\"binomial\",data=training)\ntoc() # ~4 sec\n#summary(glm_death_model)\n\ntrain_pred <- predict(glm_death_model, training[,-7], type = \"response\")\n#summary(train_pred)\n\nbinary_train_pred <- ifelse(train_pred > 0.5,1,0)\n#summary(train_pred)\n\n\nbinary_train_death <- ifelse(training$death == \"Yes\",1,0)\nlevels(binary_train_death) <- c(0,1) \nlevels(binary_train_pred) <- c(0,1) \n\nbinary_train_death <- factor(binary_train_death)\nbinary_train_pred <- factor(binary_train_pred)\n\nconfusionMatrix(binary_train_pred, reference=binary_train_death) #95% accuracy, Sensitivity : 0.980, Specificity : 0.5195 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GLM predict death\n\n# predict with hosp, without icu, & medcond\nglm_death_model_2 <- glm(death ~ factor(age) + factor(sex) + factor(race_ethnicity) + hosp  - 1, \n                  family=\"binomial\",data=training)\n\n\ntrain_pred <- predict(glm_death_model_2, training[,-7], type = \"response\")\n#summary(train_pred)\n\nbinary_train_pred <- ifelse(train_pred > 0.5,1,0)\n#summary(train_pred)\n\n\nbinary_train_death <- ifelse(training$death == \"Yes\",1,0)\nlevels(binary_train_death) <- c(0,1) \nlevels(binary_train_pred) <- c(0,1) \n\nbinary_train_death <- factor(binary_train_death)\nbinary_train_pred <- factor(binary_train_pred)\n\nconfusionMatrix(binary_train_pred, reference=binary_train_death) #94% accuracy, Sensitivity : 0.9805, Specificity : 0.3576 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"par(mfrow = c(2, 2))\nplot(glm_death_model)\npar(mfrow = c(2, 2))\nplot(glm_hosp_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Models to Test Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bayes GLM test\nbayes_test <- predict(bayes_hosp_model, testing[,], type = \"prob\")\nconfusionMatrix(predict(bayes_hosp_model, testing[,]), reference=testing$hosp) # Accuracy 83%\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"htest <- predict(glm_hosp_model, testing[,-5], type = \"response\")\n\nbinary_htest <- ifelse(htest > 0.5,1,0)\n\nbinary_testing_h <- ifelse(testing$hosp == \"Yes\",1,0)\nlevels(binary_htest) <- c(0,1) \nlevels(binary_testing_h) <- c(0,1) \n\nbinary_htest <- factor(binary_htest)\nbinary_testing_h <- factor(binary_testing_h)\n\nconfusionMatrix(binary_htest, reference=binary_testing_h) #also ~84%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1test <- predict(glm_death_model, testing[,-7], type = \"response\")\nbinary_d1test <- ifelse(d1test > 0.5,1,0)\n\nbinary_testing_d <- ifelse(testing$death == \"Yes\",1,0)\nlevels(binary_d1test) <- c(0,1) \nlevels(binary_testing_d) <- c(0,1) \n\nbinary_d1test <- factor(binary_d1test)\nbinary_testing_d <- factor(binary_testing_d)\n\nconfusionMatrix(binary_d1test, reference=binary_testing_d) # accuracy 95%\n\nd2test <- predict(glm_death_model_2, testing[,-7], type = \"response\")\nbinary_d2test <- ifelse(d2test > 0.5,1,0)\nlevels(binary_d2test) <- c(0,1) \nbinary_d2test <- factor(binary_d2test)\nconfusionMatrix(binary_d2test, reference=binary_testing_d) #also ~94%\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThe GLM models for predicting hospitalization and death all performed well on the test sets.  Specificity values ranged from around 30% to 50%.  Sensitivity values were all at least 95%.  \n\nThe GLM model is highly predictive.  ICU and medcond data is beneficial in predicting death as a patient outcome.  While it is the case that these factors were often not included in patients' data, it is slightly beneficial for predicting future patient outcomes to keep record of this data.\n\nThe GLM model from the caret package was similarly predictive to the base R GLM model, however, the latter is a signficantly faster, and therefore a better choice with this data.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction out of curiousity for my friend with a recent covid exposure:\n\nMichael_data <- testing[14750,] # matches Michael info (sex, age, race)\nMichael_data_medcond <- testing[14760,] # matches Michael + medcond to give more conservative prediction\n\nprint(\"hospitalization probabilities without and with a medical condition\")\npredict(bayes_hosp_model, Michael_data, type = \"prob\")\n\npredict(bayes_hosp_model, Michael_data_medcond, type = \"prob\")\n\nprint(\"death probabilities without and with a medical condition\")\n\npredict(glm_death_model_2, Michael_data, type = \"response\")\npredict(glm_death_model_2, Michael_data_medcond, type = \"response\") #interestingly, identical to prediction without medcond, ~0.2%\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}